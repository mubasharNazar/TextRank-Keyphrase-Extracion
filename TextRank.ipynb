{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import copy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    return unicode(''.join([i if ord(i) < 128 else ' ' for i in text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    doc = clean(text)\n",
    "\n",
    "    doc = str(doc).lower()\n",
    "    doc = nlp(clean(doc))\n",
    "    \n",
    "    # we add some words to the stop word list\n",
    "    texts, article = '', []\n",
    "    for w in doc:\n",
    "        if str(w) != '\\n':\n",
    "            texts = texts + ' ' + str(w.text)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    while i < len(texts):\n",
    "        if((ord(texts[i])>=65 and ord(texts[i]) <=90) or ord(texts[i]) == 46):\n",
    "            pass\n",
    "        elif((ord(texts[i])>=97 and ord(texts[i]) <=122) or ord(texts[i]) ==95 or ord(texts[i]) == 32):\n",
    "            pass\n",
    "        else:\n",
    "            texts = texts.replace(texts[i], '')\n",
    "        i +=1\n",
    "            #texts.remove(texts[i])\n",
    "            \n",
    "    return texts\n",
    "\n",
    "\n",
    "def noun_adj(texts):\n",
    "\n",
    "    tokens = nltk.sent_tokenize(texts)\n",
    "    #print len(tokens)\n",
    "    keyphrases = []\n",
    "    for i in range(len(tokens)):\n",
    "        tok = nltk.word_tokenize(tokens[i])\n",
    "        #print tok\n",
    "        pos = nltk.pos_tag(tok)\n",
    "        #print pos\n",
    "        #print pos\n",
    "        l, ll = '', []\n",
    "        for word, pos in pos:\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS' or pos == 'JJ' or pos == 'JJR' or pos == 'JJS'):\n",
    "                if len(l) != 0:\n",
    "                    l = l + \" \" + word\n",
    "                else:\n",
    "                    l = word\n",
    "            elif len(l) != 0:\n",
    "                ll.append(l)\n",
    "                l = ''\n",
    "        if len(l) != 0:\n",
    "            ll.append(l)\n",
    "            l = ''\n",
    "        keyphrases.append(ll)\n",
    "        ll = []\n",
    "\n",
    "    return keyphrases\n",
    "\n",
    "def mainKeyphrases(keyphrases):\n",
    "        #not w.is_stop and not w.is_punct and not w.like_num\n",
    "    #print ll\n",
    "    #print nltk.word_tokenize(ll[9])\n",
    "    mainKeys, ke, kk = [], [], []\n",
    "    #for i in range(len(keyphrases)):\n",
    "    i=0\n",
    "    while i < len(keyphrases):\n",
    "        #############\n",
    "        if len(keyphrases[i]) == 0:\n",
    "            del keyphrases[i]\n",
    "            i -= 1\n",
    "        else:\n",
    "            for j in range(len(keyphrases[i])):\n",
    "                    #print i,j\n",
    "                    #print keyphrases[i][j]\n",
    "                kk =  nltk.word_tokenize(keyphrases[i][j])\n",
    "                    #print kk\n",
    "                k=0\n",
    "                while k < len(kk):\n",
    "                    ####################3\n",
    "                    if len(kk[k]) < 3:\n",
    "                        del kk[k]\n",
    "                        k -= 1\n",
    "                    k+=1\n",
    "                            \n",
    "                ke.append(kk)\n",
    "            mainKeys.extend(ke)\n",
    "            ke = []\n",
    "        i += 1\n",
    "    #print mainKeys\n",
    "    \n",
    "    #joining keyphrases\n",
    "    keyphrase = []\n",
    "    for i in mainKeys:\n",
    "        keyphrase.append(' '.join(i))\n",
    "        \n",
    "    #removing empty items\n",
    "    i=0\n",
    "    while i < len(keyphrase):\n",
    "        if len(keyphrase[i]) < 1:\n",
    "            keyphrase.remove(keyphrase[i])\n",
    "            i-=1\n",
    "        i+=1\n",
    "    \n",
    "    return keyphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataframe():\n",
    "    #co-occur window size 2\n",
    "    for ind, i in enumerate(filtered[k]):\n",
    "        for index, j in enumerate(filtered[k]):\n",
    "            if i != j:\n",
    "                ii = ind - index\n",
    "                if ii < 2 and ii > -2:\n",
    "                    df1[i][j] = df1[i][j] + 1\n",
    "                    \n",
    "def vector_creation():\n",
    "    for i in range(0, len(df1)):\n",
    "        vector.append(1/float(len(df1)))\n",
    "    return vector\n",
    "\n",
    "def vector_update(vector):\n",
    "    vector = df1.dot(vector)\n",
    "    #np.savetxt('vector.txt', vector, fmt='%f')\n",
    "    return vector\n",
    "        \n",
    "def normalization():\n",
    "    for i, row in enumerate(df1.values):\n",
    "        summ= 0\n",
    "        for j, cols in enumerate(df1.values):\n",
    "            summ = summ + df1[df1.index[i]][df1.index[j]]\n",
    "\n",
    "        for j, cols in enumerate(df1.values):\n",
    "            df1[df1.index[i]][df1.index[j]] = df1[df1.index[i]][df1.index[j]] / summ\n",
    "            \n",
    "def FinalTextRank(vector):\n",
    "    a = 0.85\n",
    "    i = 0\n",
    "    a_sum = 0\n",
    "    #print vector\n",
    "    for df_row in df1:\n",
    "        #print df1[df_row][0]\n",
    "        a_sum = 0\n",
    "        for df_col in df1.columns:\n",
    "            summ = 0\n",
    "            if df1[df_row][df_col] != 0:\n",
    "                for dd in range(0, len(df1)):\n",
    "                    summ = summ + df1[df_col][dd]\n",
    "                a_sum = a_sum + np.multiply(df1[df_row][df_col] / summ, vector[df_col])    \n",
    "        a_sum = np.multiply(a, a_sum) \n",
    "        vector[i] = np.multiply((1-a), vector[i])\n",
    "        vector[i] = vector[i] + a_sum\n",
    "        i = i + 1\n",
    "        \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = []\n",
    "duplicates = []\n",
    "final_vector = []\n",
    "for i in range(1,2):\n",
    "    lee_train_file = \"art_and_culture.txt\"\n",
    "    text = open(lee_train_file).read()\n",
    "    texts = preprocessing(text)\n",
    "    keyphrases = noun_adj(texts)\n",
    "    filtered.append(mainKeyphrases(keyphrases))\n",
    "    duplicates.append(list(set(filtered[len(filtered)-1])))\n",
    "\n",
    "\n",
    "for k in range(0,len(duplicates)):\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    df1 = pd.DataFrame(0, index= duplicates[k], columns=duplicates[k])\n",
    "    df1 = df1.astype(float)\n",
    "    ll = df1.index.values.tolist()\n",
    "    #final_list.append(ll)\n",
    "    vector = []\n",
    "    position = []\n",
    "\n",
    "    dataframe()\n",
    "    vector = vector_creation()\n",
    "    normalization()\n",
    "    vector = vector_update(vector)\n",
    "    vector = FinalTextRank(vector)      \n",
    "    final_vector.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#top most keyphrases\n",
    "predicted_keyphrases = []\n",
    "predicted = []\n",
    "vect = copy.deepcopy(final_vector)\n",
    "for i in range(0, len(vect)):\n",
    "    try:\n",
    "        for j in range(0, 10):\n",
    "            predicted.append(vect[i].idxmax())\n",
    "            vect[i] = vect[i].drop(vect[i].idxmax())\n",
    "    except:\n",
    "        b = 0\n",
    "    predicted_keyphrases.append(predicted)\n",
    "    predicted = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['north america top grossing concert entertainers',\n",
       "  'crowd',\n",
       "  'tennis player juan monaco',\n",
       "  'thursday march',\n",
       "  'beautiful',\n",
       "  'mansion',\n",
       "  'traditional pop vocal album',\n",
       "  'purple orchids',\n",
       "  'buenos aires',\n",
       "  'media']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_keyphrases"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
